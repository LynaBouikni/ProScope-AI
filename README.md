# ProScope AI â€“ Project Document & Email Assistant

ProScope AI is an AI assistant for **project managers and consultants**.  
It ingests PDFs, Word documents, and email threads, then:

- ğŸ“š Builds a searchable knowledge base
- ğŸ§  Answers questions about project context
- ğŸ“ Summarizes long documents and meetings
- âœ… Extracts actionable tasks, owners, and deadlines
- âš ï¸ Highlights risks, blockers, and dependencies

Built with **FastAPI + RAG (Retrieval-Augmented Generation) + LLMs**.

---

## âœ¨ Features

- Upload project documents (specifications, reports, contracts, minutes, etc.)
- Upload email-like text (copy/paste important threads)
- Ask natural language questions about the project
- Get:
  - Short & long summaries
  - Key decisions
  - Extracted tasks (who does what, by when)
  - Risks & assumptions with supporting evidence
- See which document chunks were used as sources

---

## ğŸ§± Tech Stack

- **Backend:** Python, FastAPI
- **AI / NLP:** Hugging Face models, RAG pipeline (embeddings + retrieval + LLM)
- **Vector DB:** Chroma / FAISS (local, simple to run)
- **DB:** SQLite (for projects, docs & history)
- **Frontend:** Streamlit (prototype UI) or simple web client
- **Packaging & Dev:** Poetry / pip + venv, Docker, GitHub

---

## ğŸ—ï¸ Architecture Overview

1. **Ingestion**
   - User uploads a document or pastes an email thread.
   - Text is extracted (PDF â†’ text, etc.), cleaned and split into chunks.

2. **Indexing**
   - Each chunk is embedded using a sentence transformer.
   - Embeddings + metadata (doc_id, project_id, page, etc.) are stored in a vector DB.

3. **Question Answering / Analysis**
   - User asks a question or selects a mode (Summary / Actions / Risks).
   - Relevant chunks are retrieved using vector similarity.
   - A prompt is built using the retrieved context.
   - An LLM generates the answer + structured JSON if needed (e.g. tasks).

4. **Frontend**
   - User sees:
     - Final answer (summary, actions, risksâ€¦)
     - Source snippets
     - Project chat history

---

## ğŸš€ Quickstart

```bash
# 1. Clone the repo
git clone https://github.com/your-username/proscope-ai.git
cd proscope-ai

# 2. Create and activate virtual environment (example with venv)
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run backend
uvicorn app.main:app --reload

# 5. Run Streamlit UI 
streamlit run ui/app.py
```

---

## ğŸ”Œ API Endpoints (FastAPI)

POST /projects/ â€“ create a project

GET /projects/ â€“ list projects

POST /projects/{project_id}/documents/ â€“ upload a document

POST /projects/{project_id}/ask/ â€“ ask a question or request summary/actions/risks

GET /projects/{project_id}/history/ â€“ get past Q&A for this project

(See app/api for more details.)

ğŸ§ª Evaluation (Optional but Recommended)

Small synthetic QA set per project.

Compare:

LLM alone vs. RAG LLM (with retrieval).

Metrics:

Human-rated relevance, hallucination rate, task extraction quality.

---

## ğŸ›£ï¸ Roadmap

 Basic FastAPI backend & RAG pipeline

 Streamlit interface

 Multi-user authentication (optional)

 Support for Word / HTML / Markdown

 Better evaluation & logging

 Cloud deployment (Render / Railway / etc.)

---

## ğŸ“„ License

MIT


---

## ğŸ“ 3. Folder Structure

Create something like this:

```text
proscope-ai/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI entrypoint
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ projects.py      # create/list projects
â”‚   â”‚   â”œâ”€â”€ documents.py     # upload docs
â”‚   â”‚   â””â”€â”€ qa.py            # ask questions / modes
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # settings (API keys, paths)
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ db_models.py     # SQLAlchemy models for Project, Document, Chunk, Query
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic models for API
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ingestion.py     # extract text, chunking
â”‚   â”‚   â”œâ”€â”€ embeddings.py    # embed text, vector store operations
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py  # retrieval + LLM call
â”‚   â”‚   â””â”€â”€ tasks.py         # post-processing, task extraction parsing
â”‚   â””â”€â”€ db/
â”‚       â”œâ”€â”€ database.py      # DB session + init
â”‚       â””â”€â”€ migrations/      # if you use alembic later
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py               # Streamlit frontend
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_rag_pipeline.py
â”‚   â””â”€â”€ test_api.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ Dockerfile
```

## ğŸŒ 4. API Endpoints Design

You donâ€™t need all at once â€“ but hereâ€™s the spec you can grow into.

POST /projects/

Create a new project.

Body (JSON):
